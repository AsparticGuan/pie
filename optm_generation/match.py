import json
import jsonlines
import re
from tqdm import tqdm
from sentence_transformers import SentenceTransformer, util

# ---------------- å·¥å…·å‡½æ•° -------------------

def fix_and_parse_json(bad_json_str, line_no=None):
    """ä¿®å¤å¹¶è§£æéæ³• JSON"""
    def attempt_parse(s, stage):
        try:
            return json.loads(s)
        except json.JSONDecodeError as e:
            print(f"[WARN] Line {line_no}: JSON decode failed in {stage}: {e}")
            snippet = s[max(0, e.pos-50):e.pos+50]
            print("Problematic snippet:")
            print(snippet)
            return None

    data = attempt_parse(bad_json_str, stage="original")
    if data is not None:
        return data

    fixed_str = bad_json_str
    fixed_str = fixed_str.replace("\\0", "\\\\0").replace("'\0'", "'\\\\0'")
    fixed_str = re.sub(r'"\s*\n\s*"', '",\n"', fixed_str)
    fixed_str = re.sub(r'\\(?![ntr"\\u0])', r'\\\\', fixed_str)
    fixed_str = re.sub(r',\s*([}\]])', r'\1', fixed_str)
    if "'" in fixed_str and '"' not in fixed_str:
        fixed_str = fixed_str.replace("'", '"')

    data = attempt_parse(fixed_str, stage="after fixes")
    if data is not None:
        return data

    raise ValueError(f"Line {line_no}: JSON parsing failed after fixes.")

def extract_json_from_string(raw_str, line_no=None, field_name="field"):
    """
    ä»å­—ç¬¦ä¸²ä¸­æå–å¹¶è§£æ JSONï¼ˆæ”¯æŒ markdown ```json åŒ…è£¹ï¼‰
    - å°è¯•æå–ä»£ç å—å¹¶è§£æ
    - å¤±è´¥æ—¶å›é€€åˆ°åŸå§‹å­—ç¬¦ä¸²è§£æ
    - å¦‚æœä»å¤±è´¥ï¼Œè¿”å›åŸå§‹å­—ç¬¦ä¸²
    """
    if not isinstance(raw_str, str):
        return raw_str

    s = raw_str.strip()
    if not s:
        return raw_str

    # ğŸ” å°è¯•æå–ç¬¬ä¸€ä¸ª ```json ... ``` ä»£ç å—
    match = re.search(r"```json(.*?)```", s, re.DOTALL | re.IGNORECASE)
    if match:
        inner = match.group(1).strip()
    else:
        inner = s

    # å°è¯•è§£ææå–åˆ°çš„å†…å®¹
    try:
        return json.loads(inner)
    except Exception as e:
        print(f"[WARN] Failed to parse code block from {field_name} at line {line_no}: {e}")
        # å°è¯•ä¿®å¤
        try:
            return fix_and_parse_json(inner, line_no=line_no)
        except Exception:
            print(f"[INFO] Trying raw string parse for {field_name} at line {line_no}")
            # å†è¯•ç”¨åŸå§‹å­—ç¬¦ä¸²è§£æ
            try:
                return json.loads(s)
            except Exception as e2:
                print(f"[FALLBACK] Returning raw {field_name} at line {line_no}: {e2}")
                return raw_str
        
def parse_optimized_features(raw_value, line_no=None):
    """è§£æ extract_feature.jsonl ä¸­çš„ optimized_features å­—æ®µ"""
    if isinstance(raw_value, list):
        return raw_value
    return extract_json_from_string(raw_value, line_no=line_no, field_name="optimized_features")

def load_database(jsonl_path):
    """åŠ è½½ feature.jsonlï¼Œè§£æ analysis å­—æ®µ"""
    database = []
    with jsonlines.open(jsonl_path, "r") as reader:
        for idx, obj in enumerate(tqdm(reader, desc="Loading database"), start=1):
            analysis = extract_json_from_string(obj.get("analysis", ""), line_no=idx, field_name="analysis")

            if isinstance(analysis, list):
                for entry in analysis:
                    db_text = " ".join(entry.get("Unoptimized Code Conditions", []))
                    database.append({
                        "text": db_text,
                        "operation": entry.get("Optimization Operation", "")
                    })
            else:
                # å­—ç¬¦ä¸²æˆ–å…¶å®ƒ â†’ ä¿ç•™åŸå§‹
                database.append({
                    "text": str(analysis),
                    "operation": ""
                })

    print(f"\nâœ… Loaded {len(database)} entries from database")
    return database

def find_top_matches(feature, database, model, top_n=1):
    """æ‰¾åˆ°å•ä¸ªæ–°ç‰¹å¾åœ¨æ•°æ®åº“ä¸­æœ€ç›¸ä¼¼çš„å‰nä¸ªä¼˜åŒ–æ–¹æ¡ˆ"""
    db_texts = [item["text"] for item in database]
    db_embeddings = model.encode(db_texts, convert_to_tensor=True)

    feature_text = " ".join(feature["Unoptimized Code Conditions"])
    feature_embedding = model.encode(feature_text, convert_to_tensor=True)

    cos_scores = util.cos_sim(feature_embedding, db_embeddings)[0]
    top_results = cos_scores.topk(top_n)

    matches = [database[idx]["operation"] for idx in top_results.indices]

    return {
        "Unoptimized Code Conditions": feature["Unoptimized Code Conditions"],
        "Optimization Operation": matches
    }

# ---------------- ä¸»è„šæœ¬ -------------------

if __name__ == "__main__":
    # 1. åŠ è½½æ•°æ®åº“
    database = load_database("feature.jsonl")

    # 2. åŠ è½½ extract_feature.jsonl çš„åŸå§‹è¡Œ
    all_objs = []
    with jsonlines.open("extract_feature.jsonl", "r") as reader:
        all_objs = list(reader)

    # 3. åŠ è½½æ¨¡å‹
    model = SentenceTransformer("nomic-ai/nomic-embed-text-v1.5", trust_remote_code=True)

    # 4. é€è¡Œå¤„ç†ï¼Œç”Ÿæˆ analysis
    with jsonlines.open("match.jsonl", "w") as writer:
        for idx, obj in enumerate(tqdm(all_objs, desc="Processing lines"), start=1):
            raw_optimized = obj.get("optimized_features", [])
            optimized_features = parse_optimized_features(raw_optimized, line_no=idx)

            print(optimized_features)

            matches_for_line = []
            if isinstance(optimized_features, list):
                for feat in optimized_features:
                    if isinstance(feat, dict) and "Unoptimized Code Conditions" in feat:
                        result = find_top_matches(feat, database, model, top_n=1)
                        matches_for_line.append(result)

            # analysis = JSON æ•°ç»„ï¼ˆmarkdown åŒ…è£¹ï¼‰
            obj["analysis"] = "```json\n" + json.dumps(matches_for_line, indent=2, ensure_ascii=False) + "\n```"

            writer.write(obj)

    print("âœ… Matching completed. Results saved to match.jsonl, each line now has analysis")